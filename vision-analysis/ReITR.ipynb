{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong>RelTR: Relation Transformer for Scene Graph Generation</strong>\n",
    "\n",
    "This colab is mostly adapted from [here](https://colab.research.google.com/drive/1-U642OoCyb8OSM8nx9lme49dmWa_aUcU?usp=sharing#scrollTo=oB86HK9a7nyI). This colab is used to count the number of (subject, relation, object) triples in the image, independent of the caption. This helps us quantify the complexity of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "auth_token = \"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\"  # Replace with an auth token, which you can get from your huggingface account: Profile -> Settings -> Access Tokens -> New Token\n",
    "winoground = load_dataset(\"facebook/winoground\", use_auth_token=auth_token)[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some transformation functions\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "          (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im=winoground[0][\"image_0\"].convert(\"RGB\")\n",
    "plt.imshow(im)\n",
    "img = transform(im).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "Install RelTR and import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/yrcong/RelTR.git\n",
    "%cd RelTR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VG labels\n",
    "VG 150 enitiy classes and 50 relationship classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [ 'N/A', 'airplane', 'animal', 'arm', 'bag', 'banana', 'basket', 'beach', 'bear', 'bed', 'bench', 'bike',\n",
    "                'bird', 'board', 'boat', 'book', 'boot', 'bottle', 'bowl', 'box', 'boy', 'branch', 'building',\n",
    "                'bus', 'cabinet', 'cap', 'car', 'cat', 'chair', 'child', 'clock', 'coat', 'counter', 'cow', 'cup',\n",
    "                'curtain', 'desk', 'dog', 'door', 'drawer', 'ear', 'elephant', 'engine', 'eye', 'face', 'fence',\n",
    "                'finger', 'flag', 'flower', 'food', 'fork', 'fruit', 'giraffe', 'girl', 'glass', 'glove', 'guy',\n",
    "                'hair', 'hand', 'handle', 'hat', 'head', 'helmet', 'hill', 'horse', 'house', 'jacket', 'jean',\n",
    "                'kid', 'kite', 'lady', 'lamp', 'laptop', 'leaf', 'leg', 'letter', 'light', 'logo', 'man', 'men',\n",
    "                'motorcycle', 'mountain', 'mouth', 'neck', 'nose', 'number', 'orange', 'pant', 'paper', 'paw',\n",
    "                'people', 'person', 'phone', 'pillow', 'pizza', 'plane', 'plant', 'plate', 'player', 'pole', 'post',\n",
    "                'pot', 'racket', 'railing', 'rock', 'roof', 'room', 'screen', 'seat', 'sheep', 'shelf', 'shirt',\n",
    "                'shoe', 'short', 'sidewalk', 'sign', 'sink', 'skateboard', 'ski', 'skier', 'sneaker', 'snow',\n",
    "                'sock', 'stand', 'street', 'surfboard', 'table', 'tail', 'tie', 'tile', 'tire', 'toilet', 'towel',\n",
    "                'tower', 'track', 'train', 'tree', 'truck', 'trunk', 'umbrella', 'vase', 'vegetable', 'vehicle',\n",
    "                'wave', 'wheel', 'window', 'windshield', 'wing', 'wire', 'woman', 'zebra']\n",
    "\n",
    "REL_CLASSES = ['__background__', 'above', 'across', 'against', 'along', 'and', 'at', 'attached to', 'behind',\n",
    "                'belonging to', 'between', 'carrying', 'covered in', 'covering', 'eating', 'flying in', 'for',\n",
    "                'from', 'growing on', 'hanging from', 'has', 'holding', 'in', 'in front of', 'laying on',\n",
    "                'looking at', 'lying on', 'made of', 'mounted on', 'near', 'of', 'on', 'on back of', 'over',\n",
    "                'painted on', 'parked on', 'part of', 'playing', 'riding', 'says', 'sitting on', 'standing on',\n",
    "                'to', 'under', 'using', 'walking in', 'walking on', 'watching', 'wearing', 'wears', 'with']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.backbone import Backbone, Joiner\n",
    "from models.position_encoding import PositionEmbeddingSine\n",
    "from models.transformer import Transformer\n",
    "from models.reltr import RelTR\n",
    "\n",
    "position_embedding = PositionEmbeddingSine(128, normalize=True)\n",
    "backbone = Backbone('resnet50', False, False, False)\n",
    "backbone = Joiner(backbone, position_embedding)\n",
    "backbone.num_channels = 2048\n",
    "\n",
    "transformer = Transformer(d_model=256, dropout=0.1, nhead=8,\n",
    "                          dim_feedforward=2048,\n",
    "                          num_encoder_layers=6,\n",
    "                          num_decoder_layers=6,\n",
    "                          normalize_before=False,\n",
    "                          return_intermediate_dec=True)\n",
    "\n",
    "model = RelTR(backbone, transformer, num_classes=151, num_rel_classes = 51,\n",
    "              num_entities=100, num_triplets=200)\n",
    "\n",
    "# The checkpoint is pretrained on Visual Genome\n",
    "ckpt = torch.hub.load_state_dict_from_url(\n",
    "    url='https://cloud.tnt.uni-hannover.de/index.php/s/PB8xTKspKZF7fyK/download/checkpoint0149.pth',\n",
    "    map_location='cpu', check_hash=True)\n",
    "model.load_state_dict(ckpt['model'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Image\n",
    "You can replace the link with other images. Note that the entities in the used image should be included in the VG labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# propagate through the model\n",
    "obj_relations = {}\n",
    "count=0\n",
    "for sample in winoground:\n",
    "  image_0 = transform(sample[\"image_0\"].convert(\"RGB\")).unsqueeze(0)\n",
    "  image_1 = transform(sample[\"image_1\"].convert(\"RGB\")).unsqueeze(0)\n",
    "\n",
    "  for img in [image_0, image_1]:\n",
    "    outputs = model(img)\n",
    "\n",
    "    # keep only predictions with >0.3 confidence\n",
    "    probas = outputs['rel_logits'].softmax(-1)[0, :, :-1]\n",
    "    probas_sub = outputs['sub_logits'].softmax(-1)[0, :, :-1]\n",
    "    probas_obj = outputs['obj_logits'].softmax(-1)[0, :, :-1]\n",
    "    keep = torch.logical_and(probas.max(-1).values > 0.3, torch.logical_and(probas_sub.max(-1).values > 0.3,\n",
    "                                                                            probas_obj.max(-1).values > 0.3))\n",
    "    \n",
    "    # convert boxes from [0; 1] to image scales\n",
    "    sub_bboxes_scaled = rescale_bboxes(outputs['sub_boxes'][0, keep], im.size)\n",
    "    obj_bboxes_scaled = rescale_bboxes(outputs['obj_boxes'][0, keep], im.size)\n",
    "\n",
    "    topk = 10 # display up to 10 images\n",
    "    keep_queries = torch.nonzero(keep, as_tuple=True)[0]\n",
    "    indices = torch.argsort(-probas[keep_queries].max(-1)[0] * probas_sub[keep_queries].max(-1)[0] * probas_obj[keep_queries].max(-1)[0])\n",
    "    keep_queries = keep_queries[indices]\n",
    "    print(indices)\n",
    "\n",
    "    obj_relations[count] = len(indices)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('obj_relations_count.json', 'w') as fp:\n",
    "    json.dump(obj_relations, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
